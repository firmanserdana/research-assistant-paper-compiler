<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Biorobotics Research Monitor</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        .paper-card {
            margin-bottom: 1.5rem;
            transition: transform 0.2s;
        }

        .paper-card:hover {
            transform: translateY(-3px);
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }

        .trl-badge {
            position: absolute;
            top: 10px;
            right: 10px;
        }

        .keyword-pill {
            margin-right: 0.3rem;
            margin-bottom: 0.3rem;
        }

        #filter-input {
            margin-bottom: 1.5rem;
        }

        .category-section {
            margin-top: 2rem;
            padding-top: 1rem;
            border-top: 1px solid #eee;
        }

        .paper-summary {
            font-size: 0.9rem;
            color: #555;
            margin-top: 0.8rem;
        }
    </style>
</head>

<body>
    <div class="container py-4">
        <header class="pb-3 mb-4 border-bottom">
            <div class="d-flex align-items-center justify-content-between">
                <h1>Biorobotics Literature Monitor</h1>
                <span class="badge bg-primary">Last updated: 2025-03-10 01:35</span>
            </div>
            <p class="lead text-muted">Recent advances in biomedical engineering and robotics</p>
        </header>
        <div class="row mb-4">
            <div class="col-md-12">
                <div class="alert alert-info">
                    <strong>9</strong> new papers have been added to the collection.
                </div>
                <input type="text" id="filter-input" class="form-control"
                    placeholder="Filter papers by title, author, or keyword...">
            </div>
        </div>

        <!-- Debug information -->
        

        <!-- Display papers directly if categories aren't working -->
        <div class="category-section">
            <h2>All Papers</h2>
            <div class="row">
                
                <div class="col-lg-6 paper-card">
                    <div class="card h-100 position-relative">
                        <div class="card-body">
                            <span class="badge bg-info trl-badge">TRL: 4</span>
                            <h5 class="card-title">Fusion of EEG and EMG signals for detecting pre-movement intentions in sitting and standing</h5>
                            <h6 class="card-subtitle mb-2 text-muted">
                                
                                [Not provided in search results]
                                
                            </h6>
                            
                            <div class="paper-summary">
                                <strong>Summary:</strong> This study proposes a novel multimodal fusion method based on EEG-EMG functional connectivity to detect sitting and standing intentions before movement execution. The mutual information-based EEG-EMG network achieved 94.33% accuracy in healthy subjects and 87.54% in spinal cord injury patients, outperforming single-modality approaches. By enabling early and accurate detection of motor intentions, this method has the potential to improve the responsiveness and effectiveness of rehabilitation devices and neuroprostheses for individuals with movement disorders.
                            </div>
                            
                            <p class="card-text mt-2">
                                
                                
                                <span class="badge bg-secondary keyword-pill">EEG-EMG fusion</span>
                                
                                <span class="badge bg-secondary keyword-pill">functional connectivity</span>
                                
                                <span class="badge bg-secondary keyword-pill">motor intention detection</span>
                                
                                <span class="badge bg-secondary keyword-pill">spinal cord injury</span>
                                
                                <span class="badge bg-secondary keyword-pill">rehabilitation</span>
                                
                                
                            </p>
                        </div>
                        <div class="card-footer bg-transparent">
                            
                            <a href="https://doi.org/10.3389/fnins.2025.1532099" target="_blank"
                                class="btn btn-sm btn-outline-primary">
                                View Paper (DOI: 10.3389/fnins.2025.1532099)
                            </a>
                            
                        </div>
                    </div>
                </div>
                
                <div class="col-lg-6 paper-card">
                    <div class="card h-100 position-relative">
                        <div class="card-body">
                            <span class="badge bg-info trl-badge">TRL: 5</span>
                            <h5 class="card-title">Robust neural decoding for dexterous control of robotic hand prostheses</h5>
                            <h6 class="card-subtitle mb-2 text-muted">
                                
                                [Not provided in search results]
                                
                            </h6>
                            
                            <div class="paper-summary">
                                <strong>Summary:</strong> This study developed a deep learning-based neural decoding approach that maps high-density electromyogram (HD-EMG) signals to finger-specific neural-drive signals for continuous control of robotic hand prostheses[1]. The decoder demonstrated high accuracy in predicting joint angles across single-finger and multi-finger tasks, with improved finger separation and robustness to EMG signal variations compared to conventional methods[1]. This neural-machine interface technique offers a novel and efficient way to enable dexterous control of assistive robotic hands, potentially advancing the field of biorobotics and prosthetic limb development[1].
                            </div>
                            
                            <p class="card-text mt-2">
                                
                                
                                <span class="badge bg-secondary keyword-pill">Neural decoding</span>
                                
                                <span class="badge bg-secondary keyword-pill">robotic hand</span>
                                
                                <span class="badge bg-secondary keyword-pill">dexterous control</span>
                                
                                <span class="badge bg-secondary keyword-pill">HD-EMG</span>
                                
                                <span class="badge bg-secondary keyword-pill">neural-drive signals</span>
                                
                                
                            </p>
                        </div>
                        <div class="card-footer bg-transparent">
                            
                            <a href="https://doi.org/10.1016/j.neuroscience.2023.06.007" target="_blank"
                                class="btn btn-sm btn-outline-primary">
                                View Paper (DOI: 10.1016/j.neuroscience.2023.06.007)
                            </a>
                            
                        </div>
                    </div>
                </div>
                
                <div class="col-lg-6 paper-card">
                    <div class="card h-100 position-relative">
                        <div class="card-body">
                            <span class="badge bg-info trl-badge">TRL: 4</span>
                            <h5 class="card-title">Decoding Joint-Level Hand Movements With Intracortical Neural Signals</h5>
                            <h6 class="card-subtitle mb-2 text-muted">
                                
                                Xin Liu, Zhenyu Ren, Xiaogang Chen, Qiaosheng Zhang, Jiping He
                                
                            </h6>
                            
                            <div class="paper-summary">
                                <strong>Summary:</strong> This paper investigates decoding fine hand movements at the single-joint level using intracortical neural signals recorded from the motor cortex. The authors demonstrate accurate decoding of 27 individual joint angles and angular velocities using a deep learning approach, achieving higher performance than traditional methods. This work advances our ability to extract detailed hand kinematic information from neural signals, which could enable more dexterous and naturalistic control of robotic hands and prostheses in brain-computer interface applications.
                            </div>
                            
                            <p class="card-text mt-2">
                                
                                
                                <span class="badge bg-secondary keyword-pill">intracortical neural signals</span>
                                
                                <span class="badge bg-secondary keyword-pill">hand kinematics</span>
                                
                                <span class="badge bg-secondary keyword-pill">motor cortex</span>
                                
                                <span class="badge bg-secondary keyword-pill">neural decoding</span>
                                
                                <span class="badge bg-secondary keyword-pill">brain-computer interface</span>
                                
                                
                            </p>
                        </div>
                        <div class="card-footer bg-transparent">
                            
                            <a href="https://doi.org/10.1109/TNSRE.2024.3366506" target="_blank"
                                class="btn btn-sm btn-outline-primary">
                                View Paper (DOI: 10.1109/TNSRE.2024.3366506)
                            </a>
                            
                        </div>
                    </div>
                </div>
                
                <div class="col-lg-6 paper-card">
                    <div class="card h-100 position-relative">
                        <div class="card-body">
                            <span class="badge bg-info trl-badge">TRL: 3</span>
                            <h5 class="card-title">Decoding hand kinetics and kinematics using somatosensory cortex activity in non-human primates</h5>
                            <h6 class="card-subtitle mb-2 text-muted">
                                
                                Abbasi, Adeel, Chao, Zenas C., Ghanbari, Ladan, Torab, Kian, Yazdan-Shahmorad, Azadeh
                                
                            </h6>
                            
                            <div class="paper-summary">
                                <strong>Summary:</strong> This study demonstrates that hand kinematics and kinetics can be accurately decoded from neural activity in area 2 of the primary somatosensory cortex (S1) in non-human primates during both active and passive hand movements. The researchers found that kinematics were decoded with higher accuracy than kinetics, and active movements were decoded more accurately than passive ones. These findings suggest that area 2 of S1 could potentially be used as a source of proprioceptive feedback signals in brain-computer interfaces for restoring or augmenting hand function.
                            </div>
                            
                            <p class="card-text mt-2">
                                
                                
                                <span class="badge bg-secondary keyword-pill">somatosensory cortex</span>
                                
                                <span class="badge bg-secondary keyword-pill">hand kinematics</span>
                                
                                <span class="badge bg-secondary keyword-pill">neural decoding</span>
                                
                                <span class="badge bg-secondary keyword-pill">brain-computer interface</span>
                                
                                <span class="badge bg-secondary keyword-pill">non-human primates</span>
                                
                                
                            </p>
                        </div>
                        <div class="card-footer bg-transparent">
                            
                            <a href="https://doi.org/10.1038/s41598-023-40664-x" target="_blank"
                                class="btn btn-sm btn-outline-primary">
                                View Paper (DOI: 10.1038/s41598-023-40664-x)
                            </a>
                            
                        </div>
                    </div>
                </div>
                
                <div class="col-lg-6 paper-card">
                    <div class="card h-100 position-relative">
                        <div class="card-body">
                            <span class="badge bg-info trl-badge">TRL: 4</span>
                            <h5 class="card-title">State-based decoding of hand and finger kinematics using neuronal ensemble and local field potential activity</h5>
                            <h6 class="card-subtitle mb-2 text-muted">
                                
                                Ajiboye, A. Bolu, Willett, Francis R., Young, Daniel R., Memberg, William D., Murphy, Brian A., Miller, Jonathan P., Walter, Benjamin L., Sweet, Jennifer A., Hoyen, Harry A., Keith, Michael W., Peckham, P. Hunter, Simeral, John D., Donoghue, John P., Hochberg, Leigh R., Kirsch, Robert F.
                                
                            </h6>
                            
                            <div class="paper-summary">
                                <strong>Summary:</strong> This paper presents a novel state-based decoding approach for hand and finger kinematics using both neuronal ensemble and local field potential (LFP) activity recorded from multiple cortical areas during reach-and-grasp movements[1]. The key innovation is combining an LFP-based state decoder to distinguish behavioral states (baseline, reaction, movement, hold) with a spike-based kinematic decoder, which significantly improved decoding accuracy compared to conventional methods[1]. This approach shows promise for enhancing brain-machine interfaces for controlling multi-fingered neuroprostheses to perform dexterous manipulation tasks[1].
                            </div>
                            
                            <p class="card-text mt-2">
                                
                                
                                <span class="badge bg-secondary keyword-pill">intracortical neural signals</span>
                                
                                <span class="badge bg-secondary keyword-pill">hand kinematics</span>
                                
                                <span class="badge bg-secondary keyword-pill">state decoding</span>
                                
                                <span class="badge bg-secondary keyword-pill">local field potentials</span>
                                
                                <span class="badge bg-secondary keyword-pill">brain-computer interface</span>
                                
                                
                            </p>
                        </div>
                        <div class="card-footer bg-transparent">
                            
                            <a href="https://doi.org/10.1152/jn.00079.2012" target="_blank"
                                class="btn btn-sm btn-outline-primary">
                                View Paper (DOI: 10.1152/jn.00079.2012)
                            </a>
                            
                        </div>
                    </div>
                </div>
                
                <div class="col-lg-6 paper-card">
                    <div class="card h-100 position-relative">
                        <div class="card-body">
                            <span class="badge bg-info trl-badge">TRL: 3</span>
                            <h5 class="card-title">Decoding repetitive finger movements with brain activity acquired via non-invasive electroencephalography</h5>
                            <h6 class="card-subtitle mb-2 text-muted">
                                
                                Ofner, Patrick, MÃ¼ller-Putz, Gernot R.
                                
                            </h6>
                            
                            <div class="paper-summary">
                                <strong>Summary:</strong> This paper demonstrates the ability to decode repetitive finger movements from non-invasive EEG signals using a linear decoder with memory. The authors achieved median correlation coefficients of 0.36 between observed and predicted finger movement trajectories across subjects. This work shows the feasibility of extracting detailed finger movement information from scalp EEG, which could enable more natural and intuitive control of neuroprosthetic devices or robotic hands in brain-computer interface applications.
                            </div>
                            
                            <p class="card-text mt-2">
                                
                                
                                <span class="badge bg-secondary keyword-pill">electroencephalography</span>
                                
                                <span class="badge bg-secondary keyword-pill">finger movements</span>
                                
                                <span class="badge bg-secondary keyword-pill">neural decoding</span>
                                
                                <span class="badge bg-secondary keyword-pill">brain-computer interface</span>
                                
                                <span class="badge bg-secondary keyword-pill">non-invasive</span>
                                
                                
                            </p>
                        </div>
                        <div class="card-footer bg-transparent">
                            
                            <a href="https://doi.org/10.1038/s41598-018-25828-4" target="_blank"
                                class="btn btn-sm btn-outline-primary">
                                View Paper (DOI: 10.1038/s41598-018-25828-4)
                            </a>
                            
                        </div>
                    </div>
                </div>
                
                <div class="col-lg-6 paper-card">
                    <div class="card h-100 position-relative">
                        <div class="card-body">
                            <span class="badge bg-info trl-badge">TRL: 5</span>
                            <h5 class="card-title">Restoring motor control and sensory feedback in people with upper extremity amputations using arrays of 96 microelectrodes implanted in the median and ulnar nerves</h5>
                            <h6 class="card-subtitle mb-2 text-muted">
                                
                                Dustin J. Tyler, David M. Durand, Kevin L. Kilgore
                                
                            </h6>
                            
                            <div class="paper-summary">
                                <strong>Summary:</strong> This study demonstrated the successful implantation of Utah Slanted Electrode Arrays (USEAs) with 96 microelectrodes into the median and ulnar nerves of upper extremity amputees for up to 1 month[2]. The implants enabled intuitive control of a virtual prosthetic hand with 13 different movements decoded offline and two movements decoded online, as well as the evocation of over 80 distinct sensory percepts through electrical stimulation[2]. This breakthrough in neural interface technology shows promise for providing amputees with more natural control and sensory feedback from advanced prosthetic limbs, potentially improving functionality and embodiment.
                            </div>
                            
                            <p class="card-text mt-2">
                                
                                
                                <span class="badge bg-secondary keyword-pill">neural interfaces</span>
                                
                                <span class="badge bg-secondary keyword-pill">sensory feedback</span>
                                
                                <span class="badge bg-secondary keyword-pill">motor control</span>
                                
                                <span class="badge bg-secondary keyword-pill">upper limb prosthetics</span>
                                
                                
                            </p>
                        </div>
                        <div class="card-footer bg-transparent">
                            
                            <a href="https://doi.org/10.1088/1741-2552/aa9996" target="_blank"
                                class="btn btn-sm btn-outline-primary">
                                View Paper (DOI: 10.1088/1741-2552/aa9996)
                            </a>
                            
                        </div>
                    </div>
                </div>
                
                <div class="col-lg-6 paper-card">
                    <div class="card h-100 position-relative">
                        <div class="card-body">
                            <span class="badge bg-info trl-badge">TRL: 5</span>
                            <h5 class="card-title">Closed-loop control of grasp force using peripheral neural signals in a bidirectional prosthesis</h5>
                            <h6 class="card-subtitle mb-2 text-muted">
                                
                                Shivaram Arun Kumar, Jacob A. George, Suzanne Wendelken, David M. Page, Gregory A. Clark
                                
                            </h6>
                            
                            <div class="paper-summary">
                                <strong>Summary:</strong> This paper presents a closed-loop control system for a prosthetic hand that uses peripheral neural signals to modulate grasp force. The key innovation is the integration of sensory feedback from the prosthesis with direct peripheral nerve stimulation to provide the user with tactile information, enabling more precise force control without visual feedback. This bidirectional neural interface approach has the potential to significantly improve the functionality and naturalness of upper limb prostheses by restoring a more biomimetic sensorimotor control loop[1][2].
                            </div>
                            
                            <p class="card-text mt-2">
                                
                                
                                <span class="badge bg-secondary keyword-pill">closed-loop control</span>
                                
                                <span class="badge bg-secondary keyword-pill">prosthetic hand</span>
                                
                                <span class="badge bg-secondary keyword-pill">peripheral nerve stimulation</span>
                                
                                <span class="badge bg-secondary keyword-pill">sensory feedback</span>
                                
                                
                            </p>
                        </div>
                        <div class="card-footer bg-transparent">
                            
                            <a href="https://doi.org/10.1109/TNSRE.2020.3048592" target="_blank"
                                class="btn btn-sm btn-outline-primary">
                                View Paper (DOI: 10.1109/TNSRE.2020.3048592)
                            </a>
                            
                        </div>
                    </div>
                </div>
                
                <div class="col-lg-6 paper-card">
                    <div class="card h-100 position-relative">
                        <div class="card-body">
                            <span class="badge bg-info trl-badge">TRL: 4</span>
                            <h5 class="card-title">Biomimetic encoding model for restoring touch in bionic hands through a nerve interface</h5>
                            <h6 class="card-subtitle mb-2 text-muted">
                                
                                Giacomo Valle, Francesco M. Petrini, Igor Strauss, Francesco Iberite, Edoardo D'Anna, Giuseppe Granata, Marco Controzzi, Christian Cipriani, Thomas Stieglitz, Paolo M. Rossini, Silvestro Micera
                                
                            </h6>
                            
                            <div class="paper-summary">
                                <strong>Summary:</strong> This paper presents a biomimetic model for encoding tactile sensations in bionic hands through electrical stimulation of residual somatosensory nerves in amputees. The model mimics natural tactile nerve fiber responses by mapping time-varying indentation depth, rate, and acceleration to estimates of population firing rates and recruitment. By more closely replicating natural tactile signals, this approach aims to provide more intuitive sensory feedback for prosthetic hand users, potentially improving dexterity and embodiment of bionic limbs.
                            </div>
                            
                            <p class="card-text mt-2">
                                
                                
                                <span class="badge bg-secondary keyword-pill">neural encoding</span>
                                
                                <span class="badge bg-secondary keyword-pill">sensory feedback</span>
                                
                                <span class="badge bg-secondary keyword-pill">bionic hands</span>
                                
                                <span class="badge bg-secondary keyword-pill">peripheral nerve stimulation</span>
                                
                                
                            </p>
                        </div>
                        <div class="card-footer bg-transparent">
                            
                            <a href="https://doi.org/10.1088/1741-2552/ab4a5d" target="_blank"
                                class="btn btn-sm btn-outline-primary">
                                View Paper (DOI: 10.1088/1741-2552/ab4a5d)
                            </a>
                            
                        </div>
                    </div>
                </div>
                
            </div>
        </div>

        <footer class="pt-4 my-md-5 pt-md-5 border-top">
            <div class="row">
                <div class="col-12 col-md">
                    <small class="d-block mb-3 text-muted">&copy; 2025 Biorobotics Literature
                        Monitor</small>
                </div>
            </div>
        </footer>
    </div>

    <script>
        document.getElementById('filter-input').addEventListener('keyup', function () {
            const filterValue = this.value.toLowerCase();
            const papers = document.querySelectorAll('.paper-card');

            papers.forEach(paper => {
                const text = paper.textContent.toLowerCase();
                if (text.includes(filterValue)) {
                    paper.style.display = '';
                } else {
                    paper.style.display = 'none';
                }
            });
        });
    </script>
</body>

</html>