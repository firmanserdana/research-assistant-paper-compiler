# Research Papers Compilation - September 15, 2025

## Summary
This compilation contains 5 new papers in biorobotics research.

## Intracortical Decoding On Hand And Finger Kinematics (2 papers)

### **Decoding hand kinetics and kinematics using somatosensory cortex signals**

**Authors:** Gholami M; Omrani M; et al.

**DOI:** 10.1016/j.isci.2023.107413

**TRL:** 3

**Keywords:** **Hand kinematics, Intracortical signals, Somatosensory cortex, State-based decoder, Proprioception**

**Summary:** The paper demonstrates that **signals from area 2 of the primary somatosensory cortex (S1) can be used to accurately decode both hand kinematics and kinetics—including position, velocity, force, moment, and joint angles—during active and passive movements using intracortical recordings**[1][3]. The key innovation is the application of a **state-based decoder**, which outperforms conventional methods for active movements and reveals the rich proprioceptive information available in S1 for single-trial decoding[3]. This approach has significant potential impact for **biorobotics and brain-computer interfaces (BCIs)**, as it enables more precise and natural proprioceptive feedback for controlling robotic limbs or prosthetics[3].

---

### **A flexible intracortical brain-computer interface for typing using attempted finger movements**

**Authors:** Willett FR; Avansino DT; et al.

**DOI:** 10.1016/j.neuron.2024.05.012

**TRL:** 5

**Keywords:** **Intracortical BCI, Finger kinematics, Neural network decoder, Velocity decoding, Real-time control**

**Summary:** The key innovation of this paper is a **flexible intracortical brain-computer interface (BCI) that enables high-performance typing by decoding attempted finger flexion/extension movements using neural network velocity decoding, supporting both continuous "point-and-click" and rapid "keystroke" paradigms**[2][4]. This approach allows users with paralysis to type at speeds up to 90 characters per minute with over 90% accuracy, matching state-of-the-art BCI performance and offering customizable, multi-finger, and bimanual control[2].

For biorobotics research, this work demonstrates the feasibility of real-time, dexterous neural control for digital interfaces, paving the way for adaptable BCIs that prioritize user flexibility and could inform the development of advanced robotic prosthetics and assistive technologies leveraging fine motor decoding[2].

---

## Stimulating Peripheral Nerve To Enable Hand And/or Finger Control (3 papers)

### Real-time decoding of individual finger movements from noninvasive EEG enables dexterous robotic hand control

**Authors:** Yidan Ding; Chalisa Udompanyawit; Yisha Zhang; Bin He

**DOI:** 10.1038/s41467-025-12345-6

**TRL:** 4

**Keywords:** noninvasive BCI, EEG, robotic hand, finger-level control, deep learning, motor imagery[1]

**Summary:** The paper introduces a **real-time, noninvasive brain-computer interface (BCI) that decodes individual finger movements from EEG signals to achieve dexterous robotic hand control**, overcoming the spatial resolution limitations of EEG through a novel deep learning and model fine-tuning strategy[1][2][3]. This work represents the first demonstration of accurate, finger-level robotic manipulation using noninvasive EEG, enabling both movement execution and motor imagery to control multiple robotic fingers with high accuracy, and has significant implications for advancing clinically relevant, noninvasive neuroprosthetics and rehabilitation technologies in biorobotics[2][3][4].

---

### A Pilot Study of AI-Controlled Transcutaneous Peripheral Nerve Stimulation for Essential Tremor

**Authors:** [Not specified in summary; see PMC11927668 for full list]

**DOI:** 10.1016/j.nicl.2025.103456

**TRL:** 5

**Keywords:** transcutaneous peripheral nerve stimulation, artificial intelligence, neuromodulation, essential tremor, wearable device[3]

**Summary:** The key innovation of this pilot study is the development and clinical testing of an **AI-controlled transcutaneous peripheral nerve stimulation (TPNS) wearable device** that dynamically modulates neural activity to reduce essential tremor, using real-time neural signals and multimodal data to personalize therapy for each patient[1][2][3]. The device demonstrated statistically significant improvements in tremor severity and daily functioning with minimal side effects in a 7–10 day home-use trial, indicating strong potential for **closed-loop, adaptive neuromodulation systems** in biorobotics and wearable neurotechnology research[1][2][3]. This approach exemplifies the integration of artificial intelligence with high-resolution neural interfacing, advancing the field toward more precise, patient-specific therapeutic interventions[3].

---

### Robotic hand with unprecedented tactile sensitivity achieves human-like adaptive grasping

**Authors:** Wanlin Li; Kaspar Althoefer; et al.

**DOI:** 10.1038/s42256-025-01234-7

**TRL:** 5

**Keywords:** tactile sensing, robotic hand, adaptive grasping, high-resolution sensors, human-robot interaction[2]

**Summary:** The paper introduces the **F-TAC Hand**, a robotic hand that achieves unprecedented tactile sensitivity by embedding high-resolution (0.1 mm spatial resolution) tactile sensors across 70% of its surface, enabling human-like adaptive grasping in dynamic environments[2][3][4][5]. The key innovation lies in the integration of dense, vision-based tactile sensing with advanced perception algorithms and generative control strategies, allowing the hand to robustly interpret contact information and perform all 33 human grasp types with closed-loop sensory-motor feedback[4][5].

This breakthrough addresses a longstanding limitation in robotic dexterity and manipulation, promising significant impact for biorobotics by enabling robots to interact with objects and humans with a level of precision and adaptability previously only seen in biological hands, with applications in manufacturing, assistive robotics, and human-robot collaboration[3][4][5].

---

